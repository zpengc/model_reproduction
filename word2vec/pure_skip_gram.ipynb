{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pure_skip_gram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EoOjNJMWTeC",
        "outputId": "3ee6c093-88f1-4f48-d847-98b8e946347f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "CONTEXT_SIZE = 2  \n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "# 预处理\n",
        "raw_text = raw_text.lower().split()\n",
        "# 去重\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "print(\"词典大小：\", vocab_size)\n",
        "\n",
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    target = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    current = raw_text[i]\n",
        "    data.append((current, target))\n",
        "\n",
        "class SG(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SG, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        \n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, 2 * CONTEXT_SIZE * vocab_size)\n",
        "        self.activation_function = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embeddings(x).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function(out).view(2 * CONTEXT_SIZE, -1)\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1,-1)\n",
        "\n",
        "\n",
        "model = SG(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        target_vector = make_context_vector(target, word_to_ix)  \n",
        "\n",
        "        log_probs = model(torch.tensor(word_to_ix[context], dtype=torch.long))\n",
        "\n",
        "        total_loss += loss_function(log_probs, target_vector)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#TESTING\n",
        "context = 'programs'.lower()\n",
        "a = model(torch.tensor([word_to_ix[context]]))\n",
        "print(a.shape)\n",
        "print(\"current word:\", context)\n",
        "with torch.no_grad():\n",
        "  print(a)\n",
        "  top_sorted_inds = np.argsort(a, axis=1)[:,-1]\n",
        "  print(top_sorted_inds)\n",
        "  print(f'prediction word: { [ix_to_word[each.item()] for each in top_sorted_inds]}')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词典大小： 46\n",
            "torch.Size([4, 46])\n",
            "current word: programs\n",
            "tensor([[-5.1347, -5.3645, -5.5516, -5.4489, -5.4821, -4.8878, -4.8304, -5.6430,\n",
            "         -5.4174, -5.6412, -4.9771, -5.5069, -6.4267, -5.1708, -4.9736, -5.4522,\n",
            "         -5.5594, -4.8263, -5.9877, -5.3219, -6.0296, -5.8647, -5.5328, -4.8078,\n",
            "         -4.8278, -5.4324, -4.5045, -5.3767, -5.6754, -6.4788, -5.2866, -5.9858,\n",
            "         -5.6003, -6.4129, -5.4551, -5.5919, -5.1701, -6.3561, -5.2716, -5.1689,\n",
            "         -4.7864, -3.5410, -5.5445, -5.6117, -5.0596, -5.5424],\n",
            "        [-4.9652, -5.2716, -5.8952, -5.5290, -5.5970, -3.5152, -4.8213, -5.3860,\n",
            "         -5.6386, -5.2781, -5.8112, -4.9606, -5.9130, -5.8385, -5.6059, -4.5330,\n",
            "         -5.3929, -5.8497, -5.4240, -5.1187, -5.3182, -5.3124, -4.7617, -4.8988,\n",
            "         -5.5571, -5.2416, -5.4553, -5.6438, -4.8204, -5.6555, -5.5503, -5.8562,\n",
            "         -4.8738, -5.3060, -6.2866, -3.5540, -6.1209, -5.6004, -5.4876, -5.3045,\n",
            "         -5.4194, -5.3682, -5.8631, -5.4061, -5.2984, -5.9024],\n",
            "        [-5.2594, -5.3766, -5.3290, -6.0596, -5.8481, -4.6550, -5.2025, -5.9524,\n",
            "         -5.3031, -4.5969, -6.0443, -5.0977, -5.8776, -6.3909, -5.1825, -5.8220,\n",
            "         -4.9472, -3.4184, -5.1427, -4.8160, -5.9093, -5.6959, -5.2486, -6.2949,\n",
            "         -6.1480, -5.0976, -5.1914, -5.5645, -5.3878, -5.7934, -5.1346, -5.2235,\n",
            "         -5.4878, -5.3058, -4.9473, -5.5639, -5.4821, -5.3621, -6.0878, -5.5589,\n",
            "         -5.6121, -5.3275, -5.2042, -5.3611, -4.9205, -5.3524],\n",
            "        [-5.4994, -5.7218, -4.6508, -4.7811, -5.2461, -5.0207, -5.1324, -4.9427,\n",
            "         -5.4363, -5.1919, -5.2294, -5.2102, -5.9602, -5.8594, -5.2374, -4.7376,\n",
            "         -5.3609, -4.9782, -5.4948, -5.7059, -5.3364, -5.2289, -4.9274, -4.9817,\n",
            "         -5.5816, -6.6468, -5.0885, -5.5720, -4.9545, -5.0568, -5.3308, -3.7721,\n",
            "         -5.7254, -5.7444, -5.3983, -5.2583, -5.7540, -5.8469, -6.1742, -5.7730,\n",
            "         -5.1449, -4.6504, -5.5261, -5.4335, -5.6728, -5.5734]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "tensor([41,  5, 17, 31])\n",
            "prediction word: ['people', 'create', 'to', 'direct']\n"
          ]
        }
      ]
    }
  ]
}