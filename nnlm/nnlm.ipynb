{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnlm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cH9gaBQjoq4",
        "outputId": "03a8a14e-5cfb-4225-a93c-2b3c72be4331"
      },
      "source": [
        "import nltk\n",
        "import csv\n",
        "from nltk.corpus import brown  #　http://korpus.uib.no/icame/brown/bcm.html\n",
        "from nltk.corpus import wordnet  # https://wordnet.princeton.edu/\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "print(\"段落个数：\",len(brown.paras()))  # 段落\n",
        "print(\"句子个数：\",len(brown.sents()))  # 句子 \n",
        "print(\"单词个数：\",len(brown.words()))  # 单词\n",
        "print(\"原始文本长度\",len(brown.raw()))  # 原始文本"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "段落个数： 15667\n",
            "句子个数： 57340\n",
            "单词个数： 1161192\n",
            "原始文本长度 9964284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQIwPCW-nDNj",
        "outputId": "d6f5a7d0-7c2b-4838-c8ce-a25884eb4aaf"
      },
      "source": [
        "num_train = 12000  # 从15667个段落中选择的12000个段落作为训练集\n",
        "UNK_symbol = \"<UNK>\"\n",
        "vocabulary = set([UNK_symbol])  #　词典\n",
        "min_count = 5  # 单词最少出现次数\n",
        "\n",
        "brown_corpus_train = []  # 存储每个段落的单词集\n",
        "# 遍历每一个段落，解析出句子，再接着解析出单词\n",
        "for idx, paragraph in enumerate(brown.paras()):\n",
        "    if idx == num_train:\n",
        "        break\n",
        "    words = []\n",
        "    for sentence in paragraph:\n",
        "        for word in sentence:\n",
        "            words.append(word.lower())  # 可能有重复的单词\n",
        "    brown_corpus_train.append(words)\n",
        "\n",
        "\n",
        "# 统计每个单词的词频，便于构建词典\n",
        "word_term_frequency_train = {}\n",
        "for words in brown_corpus_train:\n",
        "  for word in words:\n",
        "    word_term_frequency_train[word] = word_term_frequency_train.get(word,0) + 1\n",
        "\n",
        "# 构建词典，加入词典的单词出现次数必须够多\n",
        "for words in brown_corpus_train:\n",
        "    for word in words:\n",
        "        if word_term_frequency_train.get(word,0) >= min_count:\n",
        "            vocabulary.add(word)\n",
        "\n",
        "print(\"词典大小：\",len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "词典大小： 12681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0-XoFb7p7BQ",
        "outputId": "846310e2-51c9-4f4c-99a0-83a1bbd9a841"
      },
      "source": [
        "import numpy as np\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "# 建立单词-词典索引的字典，后续传入索引代替传入字符串\n",
        "word_to_idx_mappings = {}\n",
        "for idx, word in enumerate(vocabulary):\n",
        "    word_to_idx_mappings[word] = idx\n",
        "\n",
        "# 根据单词获得词典索引，如果单词不在词典中，返回<UNK>索引,0\n",
        "def get_idx(word):\n",
        "    return word_to_idx_mappings.get(word, word_to_idx_mappings[\"<UNK>\"])\n",
        "\n",
        "# 建立训练集和测试集\n",
        "for idx, paragraph in enumerate(brown.paras()):\n",
        "    for sentence in paragraph:\n",
        "        for i, word in enumerate(sentence):\n",
        "          # trigram模型，三个连续单词为一个序列\n",
        "            if i+2 >= len(sentence):  # 序列中第三个单词无法获得\n",
        "                break\n",
        "            # trigram模型中，根据前两个预测第三个\n",
        "            x_extract = [get_idx(sentence[i].lower()), get_idx(sentence[i+1].lower())]\n",
        "            y_extract = [get_idx(sentence[i+2].lower())]\n",
        "            # 训练集\n",
        "            if idx < num_train:\n",
        "                x_train.append(x_extract)\n",
        "                y_train.append(y_extract)\n",
        "            # 测试集\n",
        "            else:\n",
        "                x_test.append(x_extract)\n",
        "                y_test.append(y_extract)\n",
        "\n",
        "# 转成numpy对象\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)  \n",
        "  \n",
        "print(\"训练集输入和输出：\", x_train.shape, y_train.shape)\n",
        "print(\"测试集输入和输出：\", x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练集输入和输出： (872823, 2) (872823, 1)\n",
            "测试集输入和输出： (174016, 2) (174016, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkz9JG2doUnR"
      },
      "source": [
        "import torch\n",
        "import multiprocessing\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time   \n",
        "\n",
        "# trigram模型，NNLM的基础\n",
        "class TrigramNNmodel(nn.Module):\n",
        "  # 在trigram中，context_size窗口大小为3-1=2，第三个单词是预测对象\n",
        "  def __init__(self, vocabulary_size, embedding_dimension, context_size, hidden_unit):\n",
        "    super(TrigramNNmodel, self).__init__()\n",
        "    self.context_size = context_size\n",
        "    self.embedding_dimension = embedding_dimension\n",
        "    self.embeddings = nn.Embedding(vocabulary_size, embedding_dimension)\n",
        "    self.linear1 = nn.Linear(context_size*embedding_dimension, hidden_unit, bias = True)  # 输入层-隐藏层\n",
        "    self.linear2 = nn.Linear(hidden_unit, vocabulary_size, bias = True)  # 隐藏层-输出层\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    x = self.embeddings(inputs).view((-1, self.context_size * self.embedding_dimension))  # shape：(1, (n-1)*m)\n",
        "    hidden_layer_output = self.linear1(x) # shape:(h, 1)\n",
        "    non_linear_output = torch.tanh(hidden_layer_output)  # shape:(h, 1)\n",
        "    output_layer_output = self.linear2(non_linear_output)  # shape:(V, 1)\n",
        "    y = F.log_softmax(output_layer_output, dim=1)  # shape:(V, 1)\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE-hmSlAyW2w",
        "outputId": "82251d17-fbcf-4bde-c909-cb2b7149d55c"
      },
      "source": [
        "gpu = 0 \n",
        "\n",
        "# hyperparameters\n",
        "EMBEDDING_DIMENSION = 200\n",
        "CONTEXT_SIZE = 2\n",
        "BATCH_SIZE = 256\n",
        "HIDDEN_UNIT = 100\n",
        "\n",
        "# 设置随机化种子，固定参数初始化，便于复现\n",
        "# https://arxiv.org/pdf/2109.08203.pdf\n",
        "torch.manual_seed(3407)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "available_workers = multiprocessing.cpu_count()\n",
        "print(\"可用的核心数：\",available_workers)\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)  # 训练集\n",
        "test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, num_workers = available_workers)\n",
        "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, num_workers = available_workers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "可用的核心数： 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piBz4fgF6deV",
        "outputId": "b79fe4fb-4353-4ef4-d3cd-6384fd40b3b4"
      },
      "source": [
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, criterion, dataloader, gpu):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        dev_st = time.time()\n",
        "        for it, data_tensor in enumerate(dataloader):\n",
        "            context_tensor = data_tensor[:,0:2]\n",
        "            target_tensor = data_tensor[:,2]\n",
        "            context_tensor, target_tensor = context_tensor.cuda(gpu), target_tensor.cuda(gpu)\n",
        "            log_probs = model(context_tensor)\n",
        "            mean_loss += criterion(log_probs, target_tensor).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
        "            count += 1\n",
        "            if it % 500 == 0: \n",
        "                print(\"Iteration {} complete. Mean Loss: {}; Mean Acc:{}; Time taken (s): {}\".format(it, mean_loss / count, mean_acc / count, (time.time()-dev_st)))\n",
        "                dev_st = time.time()\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "\n",
        "# negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIMENSION, CONTEXT_SIZE, HIDDEN_UNIT)\n",
        "model.cuda(gpu)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
        "\n",
        "\n",
        "best_acc = 0\n",
        "best_model_path = None\n",
        "print(f\"{'='*10}开始训练模型{'='*10}\")\n",
        "\n",
        "for epoch in range(5):\n",
        "    st = time.time()\n",
        "    print(\"\\ntraining epoch:{}\".format(epoch+1))\n",
        "    for _, data_tensor in enumerate(train_loader):       \n",
        "        context_tensor = data_tensor[:,0:2]  # 第一维表示样本数量，此处指batch_size个样本\n",
        "        target_tensor = data_tensor[:,2]\n",
        "\n",
        "        context_tensor = context_tensor.cuda(gpu)\n",
        "        target_tensor = target_tensor.cuda(gpu)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n",
        "\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if _ % 500 == 0: \n",
        "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Acc:{}; Time taken (s): {}\".format(_, epoch, loss.item(), acc, (time.time()-st)))\n",
        "            st = time.time()\n",
        "\n",
        "    print(f\"{'='*10}开始评估模型{'='*10}\")\n",
        "    test_acc, test_loss = evaluate(model, loss_function, test_loader, gpu)\n",
        "    print(\"Epoch {} complete! Accuracy: {}; Loss: {}\".format(epoch, test_acc, test_loss))\n",
        "    if test_acc > best_acc:\n",
        "        print(\"accuracy improved from {} to {}, saving model...\".format(best_acc, test_acc))\n",
        "        best_acc = test_acc\n",
        "        path = 'model_epoch_{}.dat'.format(epoch)\n",
        "        torch.save(model.state_dict(), path)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========开始训练模型==========\n",
            "\n",
            "training epoch:1\n",
            "Training Iteration 0 of epoch 0 complete. Loss: 9.508774757385254; Acc:0.0; Time taken (s): 0.08885073661804199\n",
            "Training Iteration 500 of epoch 0 complete. Loss: 6.249395370483398; Acc:0.16796875; Time taken (s): 3.514956474304199\n",
            "Training Iteration 1000 of epoch 0 complete. Loss: 6.142131328582764; Acc:0.140625; Time taken (s): 3.3961398601531982\n",
            "Training Iteration 1500 of epoch 0 complete. Loss: 5.987728595733643; Acc:0.1484375; Time taken (s): 3.4260268211364746\n",
            "Training Iteration 2000 of epoch 0 complete. Loss: 5.895061016082764; Acc:0.11328125; Time taken (s): 3.399430274963379\n",
            "Training Iteration 2500 of epoch 0 complete. Loss: 6.144865036010742; Acc:0.15625; Time taken (s): 3.3996307849884033\n",
            "Training Iteration 3000 of epoch 0 complete. Loss: 5.675825119018555; Acc:0.19140625; Time taken (s): 3.40194034576416\n",
            "==========开始评估模型==========\n",
            "Dev Iteration 0 complete. Mean Loss: 5.021405220031738; Mean Acc:0.1953125; Time taken (s): 0.0893411636352539\n",
            "Dev Iteration 500 complete. Mean Loss: 5.114277540804621; Mean Acc:0.17074444890022278; Time taken (s): 1.6454048156738281\n",
            "Epoch 0 complete! Accuracy: 0.16985101997852325; Loss: 5.128514190281138\n",
            "accuracy improved from 0 to 0.16985101997852325, saving model...\n",
            "\n",
            "training epoch:2\n",
            "Training Iteration 0 of epoch 1 complete. Loss: 6.295820713043213; Acc:0.13671875; Time taken (s): 0.10035490989685059\n",
            "Training Iteration 500 of epoch 1 complete. Loss: 5.670984268188477; Acc:0.19140625; Time taken (s): 3.4012911319732666\n",
            "Training Iteration 1000 of epoch 1 complete. Loss: 5.547214984893799; Acc:0.18359375; Time taken (s): 3.4083473682403564\n",
            "Training Iteration 1500 of epoch 1 complete. Loss: 5.579339504241943; Acc:0.16015625; Time taken (s): 3.3900582790374756\n",
            "Training Iteration 2000 of epoch 1 complete. Loss: 5.511048316955566; Acc:0.12890625; Time taken (s): 3.3911845684051514\n",
            "Training Iteration 2500 of epoch 1 complete. Loss: 5.482697010040283; Acc:0.171875; Time taken (s): 3.4001383781433105\n",
            "Training Iteration 3000 of epoch 1 complete. Loss: 5.263397693634033; Acc:0.2109375; Time taken (s): 3.392259359359741\n",
            "==========开始评估模型==========\n",
            "Dev Iteration 0 complete. Mean Loss: 4.964882850646973; Mean Acc:0.20703125; Time taken (s): 0.08644723892211914\n",
            "Dev Iteration 500 complete. Mean Loss: 5.092616184029037; Mean Acc:0.17440898716449738; Time taken (s): 1.6412253379821777\n",
            "Epoch 1 complete! Accuracy: 0.17364813387393951; Loss: 5.107127739401425\n",
            "accuracy improved from 0.16985101997852325 to 0.17364813387393951, saving model...\n",
            "\n",
            "training epoch:3\n",
            "Training Iteration 0 of epoch 2 complete. Loss: 5.825478553771973; Acc:0.1484375; Time taken (s): 0.09365224838256836\n",
            "Training Iteration 500 of epoch 2 complete. Loss: 5.401862621307373; Acc:0.1875; Time taken (s): 3.4027602672576904\n",
            "Training Iteration 1000 of epoch 2 complete. Loss: 5.29281759262085; Acc:0.19921875; Time taken (s): 3.393765449523926\n",
            "Training Iteration 1500 of epoch 2 complete. Loss: 5.310822010040283; Acc:0.17578125; Time taken (s): 3.390758752822876\n",
            "Training Iteration 2000 of epoch 2 complete. Loss: 5.367144584655762; Acc:0.13671875; Time taken (s): 3.3970980644226074\n",
            "Training Iteration 2500 of epoch 2 complete. Loss: 5.225642681121826; Acc:0.1796875; Time taken (s): 3.3915958404541016\n",
            "Training Iteration 3000 of epoch 2 complete. Loss: 5.014841079711914; Acc:0.23046875; Time taken (s): 3.3919756412506104\n",
            "==========开始评估模型==========\n",
            "Dev Iteration 0 complete. Mean Loss: 4.956252574920654; Mean Acc:0.19921875; Time taken (s): 0.08641195297241211\n",
            "Dev Iteration 500 complete. Mean Loss: 5.129620748127768; Mean Acc:0.1743622124195099; Time taken (s): 1.596503734588623\n",
            "Epoch 2 complete! Accuracy: 0.17353133857250214; Loss: 5.141955633023206\n",
            "\n",
            "training epoch:4\n",
            "Training Iteration 0 of epoch 3 complete. Loss: 5.549905300140381; Acc:0.15234375; Time taken (s): 0.09097909927368164\n",
            "Training Iteration 500 of epoch 3 complete. Loss: 5.191720962524414; Acc:0.203125; Time taken (s): 3.3870604038238525\n",
            "Training Iteration 1000 of epoch 3 complete. Loss: 5.098292827606201; Acc:0.21484375; Time taken (s): 3.3999485969543457\n",
            "Training Iteration 1500 of epoch 3 complete. Loss: 5.152191162109375; Acc:0.19140625; Time taken (s): 3.391832113265991\n",
            "Training Iteration 2000 of epoch 3 complete. Loss: 5.211611747741699; Acc:0.1484375; Time taken (s): 3.3842382431030273\n",
            "Training Iteration 2500 of epoch 3 complete. Loss: 5.0038628578186035; Acc:0.19140625; Time taken (s): 3.4128546714782715\n",
            "Training Iteration 3000 of epoch 3 complete. Loss: 4.834700584411621; Acc:0.2421875; Time taken (s): 3.398719310760498\n",
            "==========开始评估模型==========\n",
            "Dev Iteration 0 complete. Mean Loss: 4.999011993408203; Mean Acc:0.19921875; Time taken (s): 0.08691143989562988\n",
            "Dev Iteration 500 complete. Mean Loss: 5.179092707986127; Mean Acc:0.17333301901817322; Time taken (s): 1.6093435287475586\n",
            "Epoch 3 complete! Accuracy: 0.17288602888584137; Loss: 5.1908750407836015\n",
            "\n",
            "training epoch:5\n",
            "Training Iteration 0 of epoch 4 complete. Loss: 5.36082124710083; Acc:0.16796875; Time taken (s): 0.09827780723571777\n",
            "Training Iteration 500 of epoch 4 complete. Loss: 5.007441997528076; Acc:0.21484375; Time taken (s): 3.3969078063964844\n",
            "Training Iteration 1000 of epoch 4 complete. Loss: 4.950252532958984; Acc:0.21875; Time taken (s): 3.377417802810669\n",
            "Training Iteration 1500 of epoch 4 complete. Loss: 5.033271312713623; Acc:0.20703125; Time taken (s): 3.3783698081970215\n",
            "Training Iteration 2000 of epoch 4 complete. Loss: 5.084164142608643; Acc:0.16796875; Time taken (s): 3.3892405033111572\n",
            "Training Iteration 2500 of epoch 4 complete. Loss: 4.809467315673828; Acc:0.2109375; Time taken (s): 3.3860785961151123\n",
            "Training Iteration 3000 of epoch 4 complete. Loss: 4.665258884429932; Acc:0.2578125; Time taken (s): 3.412782669067383\n",
            "==========开始评估模型==========\n",
            "Dev Iteration 0 complete. Mean Loss: 5.041203498840332; Mean Acc:0.1953125; Time taken (s): 0.08373546600341797\n",
            "Dev Iteration 500 complete. Mean Loss: 5.234568044810952; Mean Acc:0.17197635769844055; Time taken (s): 1.6186339855194092\n",
            "Epoch 4 complete! Accuracy: 0.17155905067920685; Loss: 5.246484140087577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blgj1kP8AS5_",
        "outputId": "f0a03866-f604-46da-c2a1-3ca57e5dee8f"
      },
      "source": [
        "# 应用\n",
        "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIMENSION, CONTEXT_SIZE, HIDDEN_UNIT)\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.cuda(gpu)\n",
        "\n",
        "cos = nn.CosineSimilarity(dim=0)\n",
        "\n",
        "lm_similarities = {}\n",
        "\n",
        "word_pairs = {('computer','keyboard'),('cat','dog'),('dog','car'),('keyboard','cat')}\n",
        "\n",
        "for word_pair in word_pairs:\n",
        "    w1 = word_pair[0]\n",
        "    w2 = word_pair[1]\n",
        "    words_tensor = torch.LongTensor([get_idx(w1),get_idx(w2)])\n",
        "    words_tensor = words_tensor.cuda(gpu)\n",
        "    words_embeds = model.embeddings(words_tensor)\n",
        "    sim = cos(words_embeds[0],words_embeds[1])\n",
        "    lm_similarities[word_pair] = sim.item()\n",
        "\n",
        "print(lm_similarities)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('cat', 'dog'): 0.024955740198493004, ('computer', 'keyboard'): -0.11399353295564651, ('keyboard', 'cat'): -0.01905217580497265, ('dog', 'car'): 0.09305672347545624}\n"
          ]
        }
      ]
    }
  ]
}